{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import os\n",
    "import hashlib\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from collections import defaultdict\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,BatchNormalization,Dropout\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path\n",
    "directory_path = 'Selected/Dataset'\n",
    "\n",
    "# Use os.walk to get the directory count\n",
    "path, dirs, files = next(os.walk(directory_path))\n",
    "directory_count = len(dirs)\n",
    "\n",
    "print('Number of Directories:', directory_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path\n",
    "directory_path = 'Selected/Dataset'\n",
    "\n",
    "# Use os.walk to get the directory names\n",
    "path, dirs, files = next(os.walk(directory_path))\n",
    "\n",
    "print('Directory names:')\n",
    "for directory_name in dirs:\n",
    "    print(directory_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the subdirectory path\n",
    "subdirectory_path = 'Selected/Dataset/george'\n",
    "\n",
    "# Use os.walk to get the file count in the subdirectory\n",
    "path, dirs, files = next(os.walk(subdirectory_path))\n",
    "george_images = len(files)\n",
    "\n",
    "print('Number of images with St George :', george_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_checksum(filename):\n",
    "    hasher = hashlib.md5()\n",
    "    with open(filename, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hasher.update(chunk)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def find_duplicate_files(directory):\n",
    "    file_hashes = defaultdict(list)\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            full_path = os.path.join(root, filename)\n",
    "            file_hash = get_file_checksum(full_path)\n",
    "            file_hashes[file_hash].append(full_path)\n",
    "    \n",
    "    duplicate_groups = [files for files in file_hashes.values() if len(files) > 1]\n",
    "    return duplicate_groups\n",
    "\n",
    "directory_to_search = \"Selected/Dataset/george/\"\n",
    "duplicate_groups = find_duplicate_files(directory_to_search)\n",
    "\n",
    "for group in duplicate_groups:\n",
    "    print(\"Duplicate files:\")\n",
    "    for file_path in group:\n",
    "        print(file_path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_checksum(filename):\n",
    "    hasher = hashlib.md5()\n",
    "    with open(filename, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hasher.update(chunk)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def remove_duplicate_files(directory):\n",
    "    file_hashes = defaultdict(list)\n",
    "    removed_count = 0\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            full_path = os.path.join(root, filename)\n",
    "            file_hash = get_file_checksum(full_path)\n",
    "            file_hashes[file_hash].append(full_path)\n",
    "    \n",
    "    for files in file_hashes.values():\n",
    "        if len(files) > 1:\n",
    "            # Keep the first file, remove the rest\n",
    "            files_to_remove = files[1:]\n",
    "            for file_path in files_to_remove:\n",
    "                os.remove(file_path)\n",
    "                removed_count += 1\n",
    "    \n",
    "    return removed_count\n",
    "\n",
    "directory_to_clean = \"Selected/Dataset/george/\"\n",
    "removed_count = remove_duplicate_files(directory_to_clean)\n",
    "\n",
    "print(f\"Removed {removed_count} duplicate files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = os.listdir('Selected/Dataset/george')\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = mpimg.imread('Selected/Dataset/george/005d75fd1ab5478b0a1b3290032e9358.jpg')\n",
    "imgplt = plt.imshow(img1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the subdirectory path\n",
    "subdirectory_path = 'Selected/Dataset/no_george'\n",
    "\n",
    "# Use os.walk to get the file count in the subdirectory\n",
    "path, dirs, files = next(os.walk(subdirectory_path))\n",
    "no_george_images = len(files)\n",
    "\n",
    "print('Number of images without St George :', no_george_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_checksum(filename):\n",
    "    hasher = hashlib.md5()\n",
    "    with open(filename, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hasher.update(chunk)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def find_duplicate_files(directory):\n",
    "    file_hashes = defaultdict(list)\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            full_path = os.path.join(root, filename)\n",
    "            file_hash = get_file_checksum(full_path)\n",
    "            file_hashes[file_hash].append(full_path)\n",
    "    \n",
    "    duplicate_groups = [files for files in file_hashes.values() if len(files) > 1]\n",
    "    return duplicate_groups\n",
    "\n",
    "directory_to_search = \"Selected/Dataset/no_george/\"\n",
    "duplicate_groups = find_duplicate_files(directory_to_search)\n",
    "\n",
    "for group in duplicate_groups:\n",
    "    print(\"Duplicate files:\")\n",
    "    for file_path in group:\n",
    "        print(file_path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_checksum(filename):\n",
    "    hasher = hashlib.md5()\n",
    "    with open(filename, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hasher.update(chunk)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def remove_duplicate_files(directory):\n",
    "    file_hashes = defaultdict(list)\n",
    "    removed_count = 0\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            full_path = os.path.join(root, filename)\n",
    "            file_hash = get_file_checksum(full_path)\n",
    "            file_hashes[file_hash].append(full_path)\n",
    "    \n",
    "    for files in file_hashes.values():\n",
    "        if len(files) > 1:\n",
    "            # Keep the first file, remove the rest\n",
    "            files_to_remove = files[1:]\n",
    "            for file_path in files_to_remove:\n",
    "                os.remove(file_path)\n",
    "                removed_count += 1\n",
    "    \n",
    "    return removed_count\n",
    "\n",
    "directory_to_clean = \"Selected/Dataset/no_george/\"\n",
    "removed_count = remove_duplicate_files(directory_to_clean)\n",
    "\n",
    "print(f\"Removed {removed_count} duplicate files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = os.listdir('Selected/Dataset/no_george/')\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = mpimg.imread('Selected/Dataset/no_george/001592ee7a76aa8116c99dd35f71b841.jpg')\n",
    "imgplt = plt.imshow(img1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source folder containing images\n",
    "source_folder = 'Selected/Dataset/george/'\n",
    "\n",
    "# Get the list of all filenames in the source folder\n",
    "all_filenames = os.listdir(source_folder)\n",
    "\n",
    "# Filter the list to include only image filenames (adjust extensions as needed)\n",
    "image_filenames = [filename for filename in all_filenames if filename.lower().endswith(('.jpg', '.png'))]\n",
    "\n",
    "# Adjust the image_filenames list to contain only the first 2000 filenames\n",
    "image_filenames = image_filenames[:1000]\n",
    "\n",
    "# Destination folder to transfer images\n",
    "destination_folder = 'Selected/Train/george/'\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# Loop through the image filenames and transfer them\n",
    "for filename in image_filenames:\n",
    "    source_path = os.path.join(source_folder, filename)\n",
    "    destination_path = os.path.join(destination_folder, filename)\n",
    "    \n",
    "    # Transfer the image from source to destination\n",
    "    shutil.copy(source_path, destination_path)\n",
    "    \n",
    "    print(f\"Transferred '{filename}' to '{destination_folder}'\")\n",
    "\n",
    "print(\"Image transfer completed for all images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source folder containing images\n",
    "source_folder = 'Selected/Dataset/george/'\n",
    "\n",
    "# Get the list of all filenames in the source folder\n",
    "all_filenames = os.listdir(source_folder)\n",
    "\n",
    "# Filter the list to include only image filenames (adjust extensions as needed)\n",
    "image_filenames = [filename for filename in all_filenames if filename.lower().endswith(('.jpg', '.png'))]\n",
    "\n",
    "# Adjust the image_filenames list to contain only the first 2000 filenames\n",
    "image_filenames = image_filenames[:500]\n",
    "\n",
    "# Destination folder to transfer images\n",
    "destination_folder = 'Selected/Test/george/'\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# Loop through the image filenames and transfer them\n",
    "for filename in image_filenames:\n",
    "    source_path = os.path.join(source_folder, filename)\n",
    "    destination_path = os.path.join(destination_folder, filename)\n",
    "    \n",
    "    # Transfer the image from source to destination\n",
    "    shutil.copy(source_path, destination_path)\n",
    "    \n",
    "    print(f\"Transferred '{filename}' to '{destination_folder}'\")\n",
    "\n",
    "print(\"Image transfer completed for all images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source folder containing images\n",
    "source_folder = 'Selected/Dataset/george/'\n",
    "\n",
    "# Get the list of all filenames in the source folder\n",
    "all_filenames = os.listdir(source_folder)\n",
    "\n",
    "# Filter the list to include only image filenames (adjust extensions as needed)\n",
    "image_filenames = [filename for filename in all_filenames if filename.lower().endswith(('.jpg', '.png'))]\n",
    "\n",
    "# Adjust the image_filenames list to contain only the first 2000 filenames\n",
    "image_filenames = image_filenames[:500]\n",
    "\n",
    "# Destination folder to transfer images\n",
    "destination_folder = 'Selected/Validation/george/'\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# Loop through the image filenames and transfer them\n",
    "for filename in image_filenames:\n",
    "    source_path = os.path.join(source_folder, filename)\n",
    "    destination_path = os.path.join(destination_folder, filename)\n",
    "    \n",
    "    # Transfer the image from source to destination\n",
    "    shutil.copy(source_path, destination_path)\n",
    "    \n",
    "    print(f\"Transferred '{filename}' to '{destination_folder}'\")\n",
    "\n",
    "print(\"Image transfer completed for all images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source folder containing images\n",
    "source_folder = 'Selected/Dataset/no_george/'\n",
    "\n",
    "# Get the list of all filenames in the source folder\n",
    "all_filenames = os.listdir(source_folder)\n",
    "\n",
    "# Filter the list to include only image filenames (adjust extensions as needed)\n",
    "image_filenames = [filename for filename in all_filenames if filename.lower().endswith(('.jpg', '.png'))]\n",
    "\n",
    "# Adjust the image_filenames list to contain only the first 2000 filenames\n",
    "image_filenames = image_filenames[:1000]\n",
    "\n",
    "# Destination folder to transfer images\n",
    "destination_folder = 'Selected/Train/no_george/'\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# Loop through the image filenames and transfer them\n",
    "for filename in image_filenames:\n",
    "    source_path = os.path.join(source_folder, filename)\n",
    "    destination_path = os.path.join(destination_folder, filename)\n",
    "    \n",
    "    # Transfer the image from source to destination\n",
    "    shutil.copy(source_path, destination_path)\n",
    "    \n",
    "    print(f\"Transferred '{filename}' to '{destination_folder}'\")\n",
    "\n",
    "print(\"Image transfer completed for all images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source folder containing images\n",
    "source_folder = 'Selected/Dataset/no_george/'\n",
    "\n",
    "# Get the list of all filenames in the source folder\n",
    "all_filenames = os.listdir(source_folder)\n",
    "\n",
    "# Filter the list to include only image filenames (adjust extensions as needed)\n",
    "image_filenames = [filename for filename in all_filenames if filename.lower().endswith(('.jpg', '.png'))]\n",
    "\n",
    "# Adjust the image_filenames list to contain only the first 2000 filenames\n",
    "image_filenames = image_filenames[:500]\n",
    "\n",
    "# Destination folder to transfer images\n",
    "destination_folder = 'Selected/Test/no_george/'\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# Loop through the image filenames and transfer them\n",
    "for filename in image_filenames:\n",
    "    source_path = os.path.join(source_folder, filename)\n",
    "    destination_path = os.path.join(destination_folder, filename)\n",
    "    \n",
    "    # Transfer the image from source to destination\n",
    "    shutil.copy(source_path, destination_path)\n",
    "    \n",
    "    print(f\"Transferred '{filename}' to '{destination_folder}'\")\n",
    "\n",
    "print(\"Image transfer completed for all images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source folder containing images\n",
    "source_folder = 'Selected/Dataset/no_george/'\n",
    "\n",
    "# Get the list of all filenames in the source folder\n",
    "all_filenames = os.listdir(source_folder)\n",
    "\n",
    "# Filter the list to include only image filenames (adjust extensions as needed)\n",
    "image_filenames = [filename for filename in all_filenames if filename.lower().endswith(('.jpg', '.png'))]\n",
    "\n",
    "# Adjust the image_filenames list to contain only the first 2000 filenames\n",
    "image_filenames = image_filenames[:500]\n",
    "\n",
    "# Destination folder to transfer images\n",
    "destination_folder = 'Selected/Validation/no_george/'\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# Loop through the image filenames and transfer them\n",
    "for filename in image_filenames:\n",
    "    source_path = os.path.join(source_folder, filename)\n",
    "    destination_path = os.path.join(destination_folder, filename)\n",
    "    \n",
    "    # Transfer the image from source to destination\n",
    "    shutil.copy(source_path, destination_path)\n",
    "    \n",
    "    print(f\"Transferred '{filename}' to '{destination_folder}'\")\n",
    "\n",
    "print(\"Image transfer completed for all images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data generators for training and validation\n",
    "train_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory='Selected/Train/',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  # Change this based on your label type\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    directory='Selected/Validation/',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  # Change this based on your label type\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Print some information about the loaded datasets\n",
    "print(\"Training dataset:\")\n",
    "print(\"Number of samples:\", train_generator.samples)\n",
    "print(\"Number of classes:\", train_generator.num_classes)\n",
    "print(\"Class labels:\", train_generator.class_indices)\n",
    "print()\n",
    "\n",
    "print(\"Validation dataset:\")\n",
    "print(\"Number of samples:\", validation_generator.samples)\n",
    "print(\"Number of classes:\", validation_generator.num_classes)\n",
    "print(\"Class labels:\", validation_generator.class_indices)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory = 'Selected/Train/',\n",
    "    labels='inferred',\n",
    "    label_mode = 'int',\n",
    "    batch_size=32,\n",
    "    image_size=(256,256)\n",
    ")\n",
    "\n",
    "validation_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory = 'Selected/Validation/',\n",
    "    labels='inferred',\n",
    "    label_mode = 'int',\n",
    "    batch_size=32,\n",
    "    image_size=(256,256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "def process(image,label):\n",
    "    image = tf.cast(image/255. ,tf.float32)\n",
    "    return image,label\n",
    "\n",
    "train_ds = train_ds.map(process)\n",
    "validation_ds = validation_ds.map(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create CNN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,kernel_size=(3,3),padding='valid',activation='relu',input_shape=(256,256,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=(3,3),padding='valid',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "model.add(Conv2D(128,kernel_size=(3,3),padding='valid',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds,epochs=10,validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'],color='red',label='train')\n",
    "plt.plot(history.history['val_accuracy'],color='blue',label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'],color='red',label='train')\n",
    "plt.plot(history.history['val_loss'],color='blue',label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "test_img = cv2.imread('Selected/Test/george/009e2661284808d4373a71d2f461ac86.jpg')\n",
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = cv2.resize(test_img,(256,256))\n",
    "test_input = test_img.reshape((1,256,256,3))\n",
    "model.predict(test_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
